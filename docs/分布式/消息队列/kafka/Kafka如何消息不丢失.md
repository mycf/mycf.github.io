![[Kafka如何消息不丢失 2023-12-25 10.58.06.excalidraw|1000]]
kafka 是一个用来实现异步消息通信的一个中间件，它的整个架构是由producer consumer和broker来组成 所以可以从这三个方面来考虑和实现 
## 生产者保证消息不丢失
首先是producer端需要去确保消息能够到达broker并且实现消息的存储，有可能会出现==网络问题==，导致消息发送失败。
可以通过两种方式来避免消息丢失
第一个producer默认是异步发送消息的 那么这种情况下需要确保消息是发送成功，这里有两种方法

	1. 是把异步发送改成同步发送，那么producer就能够去实时的知道消息发送的结果。通过get()同步方法获取调用结果，但会降低业务服务的吞吐量。
	2. 添加异步回调的函数，来监听消息的发送结果，如果发送失败，可以在回调中去进行重试
第二个 producer本身提供了一个重试参数叫retries，如果因为网络问题或者broker故障 导致发送失败 那么producer会自动重试。在一致性不高的场景可以使用，设置一个比较合理的值，一般是3。另外，建议将==重试间隔设置长一些==，因为间隔时间太小，可能一次网络波动的时间重试全部结束了。


## BROKER消息不丢失  
broker端需要确保producer发送过来的消息是不会丢失的，只需要去把这个消息==持久化到磁盘==就可以了
Kafka 为了提升性能采用了异步批量刷盘的实现机制，按照一定的消息量和时间间隔去刷盘，而最终刷新到磁盘这个动作是由操作系统来调度的，所以如果在刷盘之前系统崩溃了，就会导致数据丢失，kafka并没有提供同步刷盘的一个实现机制，所以针对这个问题，==需要通过partition的副本机制和acks机制来解决==，
简单说一下partition的副本机制：它是针对每个数据分区的高可用策略，每一个patition副本集会包含唯一的一个leader 和多个follower，leader 专门去处理事务类型的请求，而follower去负责同步leader的数据，那么在这样一个机制的基础上，kafka 提供了一个acks的一个参数，通过设置acks参数，去结合broker的副本机制 来共同保障数据的可靠性，acks这个参数的值有三个选择 :
- 0：表示producer不需要等待broker的消息确认。这个选项==时延最小但同时风险最大==（因为当server宕机时，数据将会丢失）。
- 1 ，生产者把消息发送到leader副本，leader副本在成功写入到本地日志之后就告诉生产者消息提交成功，但是如果ISR集合中的follower副本还没来得及同步leader副本的消息，leader挂了，就==会造成消息丢失==。
- -1或all ，消息不仅仅写入到leader副本，并且被ISR集合中所有副本同步完成之后才告诉生产者已经提交成功，这个时候即使leader副本挂了也==不会造成数据丢失==。

> [!NOTE] 热门配置
典型的场景是创建一个复制因子`replication.factor`为 3 的主题，设置最小同步副本数
`min.insync.replicas`为 2，并使用“all”的 ack 进行生成。这将确保生产者在大多数副本未收到写入时引发异常。


**一、分区副本**
你可以创建更多的分区来提升可靠性，但是分区数过多也会带来性能上的开销，一般来说，3个副本就能满足对大部分场景的可靠性要求。
  

> [!NOTE] acks=all就可以代表数据一定不会丢失了吗？
所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。 这样才能保证写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。


## 消费端消息不丢失

保障消息到了broker之后，消费者也需要有一定的保证，因为消费者也可能出现某些问题导致消息没有消费到。

`enable.auto.commit`默认为true，也就是自动提交offset，自动提交是批量执行的，有一个时间窗口，这种方式会带来重复提交或者消息丢失的问题，所以对于高可靠性要求的程序，要使用手动提交。对于高可靠性要求的应用来说，宁愿重复消费也不应该因为消费异常而导致消息丢失。


### 异常导致的数据丢失

单条数据的长度超过限制会丢失数据，报kafka.common.MessageSizeTooLargeException异常，导致生产者消息积压，内存上升。

解决方法：
修改Kafka Broker的配置，修改单条消息的最大长度、单条消息的最大长度等参数配置。









### Kafka系统内丢失消息的情况

假如leader副本所在的broker突然挂掉，那么就要从follower副本重新选出一个leader，但是leader的数据还有一些没有被follower副本的同步的话，就会造成消息丢失。

解决方法：

为了减少Kafka系统内丢失消息的情况，Kafka需要配置如下几个参数：

1. Producer端设置`acks`  
    =all。`acks`  
    的默认值为1，代表消息被leader副本接收之后就算被成功发送。当配置`acks`  
    =all代表则所有副本都要接收到该消息之后该消息才算真正成功被发送。（副本只是将消息存储在PageCache上的，定期flush到磁盘上的，如果出现断电或者机器故障等，PageCache上的数据就丢失了。但设置设置了`acks`  
    =all，出现多个副本同时挂掉的概率比Leader挂掉的概率就小很多)
    
2. topic设置`replication.factor`  
>=3。为了保证leader副本能有follower 副本能同步消息，一般会设置`replication.factor`  >=3。这样就可以保证每个分区(partition)至少有3个副本。虽然造成了数据冗余，但是带来了数据的安全性。
    
3. 设置`min.insync.replicas`  
    >1。一般情况下需要设置`min.insync.replicas`  
    >1，这样配置代表消息至少要被写入到2个副本才算是被成功发送（默认值为1）。在实际生产中应尽量避免`min.insync.replicas`  
    值为1，此外，为了保证整个Kafka服务的高可用性，你需要确保`replication.factor`  
    >`min.insync.replicas`  
    ，否则有一个副本挂掉，整个分区就无法正常工作了。推荐设置成`replication.factor`  
    =`min.insync.replicas`  
    +1。
    
4. 设置`unclean.leader.election.enable`  
    =false。即不允许Unclean leader选举。
    
5. Producer端设置`retries`  
    。配合`acks`  
    =all，这样可以保证leader挂掉之后，Producer会重新发送消息。
    

> Unclean leader选举：Kafka把不在ISR列表中的存活副本称为“非同步副本”，这些副本中的消息远远落后于leader，如果选举这种副本作为leader的话就可能造成数据丢失。Kafka broker端提供了一个参数unclean.leader.election.enable，用于控制是否允许非同步副本参与leader选举；如果开启，则当ISR为空时就会从这些副本中选举新的leader，这个过程称为Unclean leader选举。



kafka 是一个用来实现异步 消息通信的一个中间件 它的整个架构是由producer consumer和brock来组成 所以啊对于kafka 如何去保证消息不丢失 这个问题呢 我认为可以从三个方面来考虑和实现 
- 首先是producer端 需要去确保消息能够到达broker 并且实现消息的存储 在这个层面上 有可能会出现网络问题 导致消息发送失败 所以呢针对pro端 可以通过两种方式来避免消息丢失 第一个producer默认是异步发送消息的 那么这种情况下需要确保消息是发送成功 那么这里面有两个方法 第一个是把异步发送改成同步发送 那么这样的话呢 producer就能够去实时的知道 消息发送的一个结果 第二种是添加异步回调的函数 来监听消息的发送结果 如果发送失败 可以在回调中去进行从事 第三个啊 producer本身提供了一个从事参数叫retrs 如果因为网络问题或者broker故障 导致发送失败 那么producer会自动从事 
- 然后是broker端需要确保producer发送过来的消息是不会丢失的 也就是说 只需要去把这个消息持久化到磁盘就可以了 但是啊kafka 为了提升性能 采用了异步批量刷盘的实现机制 也就是说按照一定的消息量和时间间隔去刷盘 而最终刷新到磁盘 这个动作是由操作系统来调度的 所以如果在刷盘之前系统崩溃了 就会导致数据丢失 kafka并没有提供同步刷盘的一个实现机制 所以针对这个问题 需要通过partition的副本机制 和acks机制来解决 我简单说一下partition的副本机制啊 它是针对每个数据分区的高可用策略 每一个petition副本集会包含唯一的一个leader 和多个follow leader 专门去处理事务类型的请求 而follow呢去负责同步leader的数据 那么在这样一个机制的基础上呢 kafka 提供了一个acks的一个参数 producer可以去设置acks参数 去结合broker的副本机制 来共同保障数据的可靠性 acks这个参数的值呢有几个选择 第一个是hk s等于零 表示producer不需要等待broker的响应 就认为消息就发送成功了 那么这种情况下会存在消息丢失 前面已经讲过了 第二个是s k s等于一 表示啊block中的leader partition收到消息之后 不等待其他的follow petition的同步 就给producer返回了一个确认 这种情况下啊 假设leader partition挂了 就会存在数据丢失 第三个是a c k s等于-1 表示broke中的leader petition收到消息之后 并且啊等待rs 2列表中的所有follow同步完成 再去给producer返回一个确认 那么这样一个配置是可以保证数据的可靠性的
- 最后就是consumer必须要能够消费的这个消息 实际上我认为啊 只要producer和broker的消息可靠性得到了保障 那么消费端是不太可能出现 消息无法消费的问题的 除非是说没有消费完这个消息就已经提交了这样一个offset 但是即便是出现这样一个情况 我们也可以通过重新调整offset的值 来实现重新消费



生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。

解决方法：
不能认为在调用send方法发送消息之后消息消息发送成功了。为了确定消息是发送成功，需要判断消息发送的结果。但要注意的是Kafka生产者(Producer) 使用send方法发送消息实是异步的操作，虽然可以优化的方式是改为回调函数的形式。

此外，对于一致性要求不高的业务场景，可以考虑Producer端设置`retries` 重试次数）设置一个比较合理的值，一般是3。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议将重试间隔设置长一些，因为间隔时间太小，可能一次网络波动的时间重试全部结束了。

二、[acks](https://kafka.apache.org/documentation/#producerconfigs_acks)
生产者发送消息的可靠性，也就是我要保证我这个消息一定是到了broker并且完成了多副本的持久化，但这种要求也同样会带来性能上的开销。它有几个可选项:

- 1 ，生产者把消息发送到leader副本，leader副本在成功写入到本地日志之后就告诉生产者消息提交成功，但是如果isr集合中的follower副本还没来得及同步leader副本的消息，leader挂了，就会造成消息丢失
    
- -1 ，消息不仅仅写入到leader副本，并且被ISR集合中所有副本同步完成之后才告诉生产者已经提交成功，这个时候即使leader副本挂了也不会造成数据丢失。
    
- 0：表示producer不需要等待broker的消息确认。这个选项时延最小但同时风险最大（因为当server宕机时，数据将会丢失）。



### 消费者丢失消息的情况

自动提交开启会存在这样的问题：当消费者poll到这个消息，还没进行真正消费的时候，offset被自动提交的同时消费者挂掉了。

解决办法：

关闭自动提交offset（即：`enable.auto.commit`  
为false），每次在真正消费完消息之后，手动提交offset。

但这样还是会存在消费者刚消费完消息，还没提交offset，结果宕机了，那么这个消息理论上就会被消费两次，因此消费端幂等性是需要保证。可以查看博客《一文理解如何实现接口的幂等性》，有这种问题对应的解决方案
