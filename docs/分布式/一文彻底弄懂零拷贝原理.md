## 零拷贝

零拷贝（Zero-Copy）是一种 `I/O` 操作优化技术，可以快速高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间。其在 `FTP` 或者 `HTTP` 等协议中可以显著地提升性能。但是需要注意的是，并不是所有的操作系统都支持这一特性，目前只有在使用 `NIO` 和 `Epoll` 传输时才可使用该特性。

需要注意，它不能用于实现了数据加密或者压缩的文件系统上，只有传输文件的原始内容。这类原始内容也包括加密了的文件内容。

## 传统I/O操作存在的性能问题

如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。

传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。

代码通常如下，一般会需要两个系统调用：

```c
read(file, tmp_buf, len); 
write(socket, tmp_buf, len);
```

代码很简单，虽然就两行代码，但是这里面发生了不少的事情。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f6c3a1a5de3640aeb3b8a8771ff3a810~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

首先，期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。

上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。

其次，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：

- `第一次拷贝`，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
- `第二次拷贝`，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
- `第三次拷贝`，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
- `第四次拷贝`，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。

这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。

所以，**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数**。

## 零拷贝技术原理

零拷贝主要是用来解决操作系统在处理 I/O 操作时，频繁复制数据的问题。关于零拷贝主要技术有 `mmap+write`、`sendfile`和`splice`等几种方式。

### 虚拟内存

在了解零拷贝技术之前，先了解虚拟内存的概念。

所有现代操作系统都使用虚拟内存，使用虚拟地址取代物理地址，主要有以下几点好处：

- 多个虚拟内存可以指向同一个物理地址。
- 虚拟内存空间可以远远大于物理内存空间。

利用上述的第一条特性可以优化，可以把内核空间和用户空间的虚拟地址映射到同一个物理地址，这样在 I/O 操作时就不需要来回复制了。

如下图展示了虚拟内存的原理。

![image-20210812181924274](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f93635b183ef49828843c0f50518449a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

### mmap/write 方式

使用`mmap/write`方式替换原来的传统I/O方式，就是利用了虚拟内存的特性。下图展示了`mmap/write`原理：

![image-20210812201839908](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d3747aca11884a1a85708c0163c79a99~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

整个流程的核心区别就是，把数据读取到内核缓冲区后，应用程序进行写入操作时，直接把内核的`Read Buffer`的数据复制到`Socket Buffer`以便写入，这次内核之间的复制也是需要CPU的参与的。

上述流程就是少了一个 CPU COPY，提升了 I/O 的速度。不过发现上下文的切换还是4次并没有减少，这是因为还是要应用程序发起`write`操作。

> 那能不能减少上下文切换呢?这就需要`sendfile`方式来进一步优化了。

### sendfile 方式

从 Linux 2.1 版本开始，Linux 引入了 `sendfile`来简化操作。`sendfile`方式可以替换上面的`mmap/write`方式来进一步优化。

`sendfile`将以下操作：

java

复制代码

  `mmap();   write();`

`替换为：`

java

复制代码

 `sendfile();`

这样就减少了上下文切换，因为少了一个应用程序发起`write`操作，直接发起`sendfile`操作。

下图展示了`sendfile`原理：

![image-20210812201905046](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d221a3a90a754ca9842f6324455638ea~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

`sendfile`方式只有三次数据复制（其中只有一次 CPU COPY）以及2次上下文切换。

> 那能不能把 CPU COPY 减少到没有呢？这样需要带有 `scatter/gather`的`sendfile`方式了。

### 带有 scatter/gather 的 sendfile方式

Linux 2.4 内核进行了优化，提供了带有 `scatter/gather` 的 sendfile 操作，这个操作可以把最后一次 `CPU COPY` 去除。其原理就是在内核空间 Read BUffer 和 Socket Buffer 不做数据复制，而是将 Read Buffer 的内存地址、偏移量记录到相应的 Socket Buffer 中，这样就不需要复制。其本质和虚拟内存的解决方法思路一致，就是内存地址的记录。

下图展示了scatter/gather 的 sendfile 的原理：

![image-20210812201922193](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/133430c1aedc4e22a6e340efc29e4239~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

scatter/gather 的 sendfile 只有两次数据复制（都是 DMA COPY）及 2 次上下文切换。CUP COPY 已经完全没有。不过这一种收集复制功能是需要硬件及驱动程序支持的。

### splice 方式

`splice` 调用和`sendfile` 非常相似，用户应用程序必须拥有两个已经打开的文件描述符，一个表示输入设备，一个表示输出设备。与`sendfile`不同的是，`splice`允许任意两个文件互相连接，而并不只是文件与`socket`进行数据传输。对于从一个文件描述符发送数据到`socket`这种特例来说，一直都是使用`sendfile`系统调用，而`splice`一直以来就只是一种机制，它并不仅限于`sendfile`的功能。也就是说 sendfile 是 splice 的一个子集。

在 Linux 2.6.17 版本引入了 splice，而在 Linux 2.6.23 版本中， sendfile 机制的实现已经没有了，但是其 API 及相应的功能还在，只不过 API 及相应的功能是利用了 splice 机制来实现的。

和 sendfile 不同的是，splice 不需要硬件支持。

## 总结

无论是传统的 I/O 方式，还是引入了零拷贝之后，2 次 `DMA copy`是都少不了的。因为两次 DMA 都是依赖硬件完成的。所以，所谓的零拷贝，都是为了减少 CPU copy 及减少了上下文的切换。

下图展示了各种零拷贝技术的对比图：

|                     | CPU拷贝 | DMA拷贝 | 系统调用   | 上下文切换 |
| ------------------- | ------- | ------- | ---------- | ---------- |
| 传统方法            | 2       | 2       | read/write | 4          |
| 内存映射            | 1       | 2       | mmap/write | 4          |
| sendfile            | 1       | 2       | sendfile   | 2          |
| scatter/gather copy | 0       | 2       | sendfile   | 2          |
| splice              | 0       | 2       | splice     | 0          |


















# 零拷贝技术第一篇：综述

**零拷贝**(zero copy)在一些语境下指代的意思有所不同,本文讲的零拷贝就是大家常说的，通过这个技术让CPU释放出来不去执行内存中数据拷贝的功能，或者避免不必要的拷贝，所以说零拷贝不是没有数据的拷贝(复制)，而是广义上讲的减少和避免不必要的数据拷贝,可以用来节省CPU使用和内带宽等，比如通过网络高速传输文件、实现网络proxy等等，零拷技术可以极大的提高程序的性能。

本文总结零拷贝的各种技术，下一篇介绍常见的零拷贝技术在Go语言中的应用。

## 零拷贝技术

其实，零拷贝很久以来都被用在提升程序的性能上，比如nginx、kafka等，而且很多文章也详细介绍了零拷贝就要解决的问题，我在这里还是在总结一下，如果你已经了解了零拷贝的计数，不妨回顾一下。

我们来分析一个从网络读取文件的场景。服务器从磁盘读取一个文件，并写入到socket中返回给客户端。我们看看服务端的数据拷贝情况：  
[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/readwrite.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/readwrite.png)

程序开始使用系统调用[read](https://man7.org/linux/man-pages/man2/read.2.html)告诉操作系统要从磁盘文件中读取数据，它首先从用户态切换到内核态，这个切换是有花费的，操作系统需要保存用户态的状态，一些寄存器的地址等，等read系统调用完成后返回，程序又需要从内核态切换到用户态，把保存的用户态的状态恢复，所以一次系统调用需要两次的用户态/内核态的切换。同样，把文件的内容写入到socket的时候，程序调用[write](https://man7.org/linux/man-pages/man2/write.2.html)系统调用，又进行了两次用户态/内核态的切换。

从操作的数据来看，这个数据还被拷贝了四次。在read系统调用的时候，DMA方式从磁盘拷贝到内核缓冲区，又通过CPU拷贝从内核缓冲区拷贝到用户的程序缓冲区，这里发生了两次拷贝。在写入socket的时候，数据先从用户程序缓冲区写入到socket缓冲区，又通过DMA方式从socket缓冲区写入到网卡。数据拷贝也发生了四次。

> DMA(Direct Memory Access，直接存储器访问) 是计算机科学中的一种内存访问技术。它允许某些电脑内部的硬件子系统（电脑外设），可以独立地直接读写系统内存，允许不同速度的硬件设备来沟通，而不需要依于中央处理器的大量中断负载。

你可以看到，传统的IO读写方式，包括了四次用户态/内核态的上下文切换，四次数据的拷贝，对性能的影响还是挺大的。广义的零拷贝的技术，就是要尽量减少用户态/内核态的上下文切换，以及数据的拷贝次数，为此操作系统也提供了几种方法。

### mmap + write

通过mmap系统调用，将用户空间的虚拟地址和内核空间的虚拟地址映射成同一个物理地址这样可以减少内核空间和内核空间的数据拷贝。

[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/mmap.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/mmap.png)

通过mmap系统调用发起IO读取，DMA将磁盘数据写入到内核缓冲区，此时mmap系统调用就返回了。程序调用write系统调用，CPU将内核缓冲区的数据写入到socket缓冲区，DMA又将数据从socket缓冲区谢瑞到网卡。

可以看到，mmap+write方式有两次系统调用，发生四次用户态/内核态的切换，三次数据拷贝。

相对传统的IO方式，减少了一次数据拷贝，但是应该还有优化的空间。

### sendfile

[sendfile](https://man7.org/linux/man-pages/man2/sendfile.2.html)是Linux2.1内核版本后引入的一个系统调用函数,用来优化数据传输。它可以在文件描述符之间传递数据，因为都是在内核之间传递数据，所以非常高效。  
Linux 2.6.33之前目的文件描述符必须是socket，以后的版本就没有限制了，可以是任意的文件。

但是源文件描述符要求必须是支持[mmap](https://man7.org/linux/man-pages/man2/mmap.2.html)操作的文件描述符，普通的文件可以，但是socket就不行了。所以sendfile适合从文件读取数据写socket场景，所以**sendfile**这个名字还是很贴切的，发送文件。

[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/sendfile.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/sendfile.png)

用户调用sendfile系统调用，数据通过DMA拷贝到内核缓冲区，CPU将数据从内核缓冲区再写入到socket缓冲区，DMA将socket缓冲区数据写入到网卡，然后sendfile系统调用返回。

可以看到，这里只有一次系统调用，也就是两次用户态/内核态的切换，三次数据拷贝。

相对来说，这种方式对性能已经有所提升。

linux 2.4之后，又对sendfile做了优化，对于支持 dms scatter/gather功能的网卡，只把关于数据的位置和长度的信息的描述符被追加到了socket缓冲区中。DMA引擎直接把数据从内核缓冲区传输到网卡(protocol engine），从而消除了仅有的一次CPU拷贝。

[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/sg-dma.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/sg-dma.png)

### splice、tee、vmsplice

sendfile性能虽好，但是还是有些场景下是不能使用的，比如我们想做一个socket proxy,源和目的都是socket,就不能直接使用sendfile了。这个时候我们可以考虑[splice](https://man7.org/linux/man-pages/man2/splice.2.html)。

Linux 2.6.30版本之前，源和目的只能有一个是管道(pipe), 自2.6.31开始, 源和目的只要保证有一个是就行。

[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/splice.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/splice.png)

当然，如果我们处理的源和目的不是管道的话，我们可以先建立一个管道，这样就可以使用splice系统调用来实现零拷贝了。

但是，如果每次都创建一个管道，你会发现每次都会多一次系统调用，也就是两次用户态/内核态的切换，所以你如果频繁的拷贝数据，那么可以建立一个管道池就像潘建给Go的标准库提供的一个补丁一样，利用pipe pool对Go语言中的splice做了优化。

tee系统调用用来在两个管道中拷贝数据。  
vmsplice系统调用pipe指向的内核缓冲区和用户程序的缓冲区之间的数据拷贝。

### MSG_ZEROCOPY

Linux v4.14 版本接受了在TCP send系统调用中实现的支持零拷贝([MSG_ZEROCOPY](https://www.kernel.org/doc/html/v4.17/networking/msg_zerocopy.html))的patch，通过这个patch，用户进程就能够把用户缓冲区的数据通过零拷贝的方式经过内核空间发送到网络套接字中去，在5.0中支持UDP。Willem de Bruijn 在他的论文里给出的压测数据是：采用 netperf 大包发送测试，性能提升 39%，而线上环境的数据发送性能则提升了 5%~8%，官方文档陈述说这个特性通常只在发送 10KB 左右大包的场景下才会有显著的性能提升。一开始这个特性只支持 TCP，到内核 v5.0 版本之后才支持 UDP。这里也有一篇官方文档介绍:[Zero-copy networking](https://lwn.net/Articles/726917/)

首先你需要设置socket选项:

|   |   |
|---|---|
|1<br><br>2|if (setsockopt(fd, SOL_SOCKET, SO_ZEROCOPY, &one, sizeof(one)))<br><br>        error(1, errno, "setsockopt zerocopy");|

然后调用send系统调用是传入`MSG_ZEROCOPY`参数:

|   |   |
|---|---|
|1|ret = send(fd, buf, sizeof(buf), MSG_ZEROCOPY);|

这里我们传入了buf,但是啥时候buf可以重用呢？这个内核会通知程序进程。它将完成通知放在socket error队列中，所以你需要读取这个队列，知道拷贝啥时候完成buf可释放或者重用了:

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10|pfd.fd = fd;<br><br>pfd.events = 0;<br><br>if (poll(&pfd, 1, -1) != 1 \| pfd.revents & POLLERR == 0)<br><br>        error(1, errno, "poll");<br><br>ret = recvmsg(fd, &msg, MSG_ERRQUEUE);<br><br>if (ret == -1)<br><br>        error(1, errno, "recvmsg");<br><br>read_notification(msg);|

因为它可能异步发送数据，你需要检查buf啥时候释放，增加代码复杂度，以及会导致多次用户态和内核态的上下文切换；

Linux 4.18中也支持的receive MSG_ZEROCOPY机制([Zero-copy TCP receive](https://lwn.net/Articles/752188/)).

字节跳动的同学2021年10曾写过文章，通过修改内核的方式兼容先前的send调用方式。这毕竟是特殊的优化，不适合大众的使用方式，所以这个零拷贝的方式还是只在一些特殊的场景下进行优化：

> 字节跳动框架组和字节跳动内核组合作，由内核组提供了同步的接口：当调用 sendmsg 的时候，内核会监听并拦截内核原先给业务的回调，并且在回调完成后才会让 sendmsg 返回。 这使得我们无需更改原有模型，可以很方便地接入 ZeroCopy send。同时，字节跳动内核组还实现了基于 unix domain socket 的 ZeroCopy，可以使得业务进程与 Mesh sidecar 之间的通信也达到零拷贝。
> 
> [字节跳动在 Go 网络库上的实践](https://www.cloudwego.io/zh/blog/2021/10/09/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/)

### copy_file_range

Linux 4.5 增加了一个新的API: [copy_file_range](https://man7.org/linux/man-pages/man2/copy_file_range.2.html), 它在内核态进行文件的拷贝，不再切换用户空间，所以会比cp少块一些，在一些场景下会提升性能。  
[![](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/copy_file_range.png)](https://colobu.com/2022/11/19/zero-copy-and-how-to-use-it-in-go/copy_file_range.png)

## 其它

[AF_XDP](https://lwn.net/Articles/750845/)是Linux 4.18新增加的功能，以前称为AF_PACKETv4（从未包含在主线内核中），是一个针对高性能数据包处理优化的原始套接字，并允许内核和应用程序之间的零拷贝。由于套接字可用于接收和发送，因此它仅支持用户空间中的高性能网络应用。

当然零拷贝技术和数据拷贝的优化一直是大家追求性能优化的方式之一，相关技术也在不断研究之中，欢迎在原文的评论中写出你的看法。

##参考文章  
以下文章是我整理的关于零拷贝技术一部分文章，如果你想深入了解零拷贝技术，可以阅读这些更多的文章。

1. [https://www.zhihu.com/question/35093238?utm_id=0](https://www.zhihu.com/question/35093238?utm_id=0)
2. [https://strikefreedom.top/archives/pipe-pool-for-splice-in-go](https://strikefreedom.top/archives/pipe-pool-for-splice-in-go)
3. [https://www.modb.pro/db/212924](https://www.modb.pro/db/212924)
4. [https://blog.lpflpf.cn/passages/golang-zerocopy/](https://blog.lpflpf.cn/passages/golang-zerocopy/)
5. [https://medium.com/swlh/linux-zero-copy-using-sendfile-75d2eb56b39b](https://medium.com/swlh/linux-zero-copy-using-sendfile-75d2eb56b39b)
6. [https://www.cloudwego.io/zh/blog/2021/10/09/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/#zerocopy](https://www.cloudwego.io/zh/blog/2021/10/09/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/#zerocopy)
7. [https://zhuanlan.zhihu.com/p/360343446](https://zhuanlan.zhihu.com/p/360343446)
8. [https://blog.devgenius.io/linux-zero-copy-d61d712813fe](https://blog.devgenius.io/linux-zero-copy-d61d712813fe)
9. [https://www.kernel.org/doc/html/v4.18/networking/msg_zerocopy.html](https://www.kernel.org/doc/html/v4.18/networking/msg_zerocopy.html)
10. [https://lwn.net/Articles/879724/](https://lwn.net/Articles/879724/)
11. [https://www.phoronix.com/news/Linux-5.20-IO_uring-ZC-Send](https://www.phoronix.com/news/Linux-5.20-IO_uring-ZC-Send)
12. [https://en.wikipedia.org/wiki/Zero-copy](https://en.wikipedia.org/wiki/Zero-copy)
13. [https://aijishu.com/a/1060000000149804](https://aijishu.com/a/1060000000149804)
14. [https://github.com/golang/go/issues/48530](https://github.com/golang/go/issues/48530)
15. [https://juejin.cn/post/6863264864140935175](https://juejin.cn/post/6863264864140935175)
16. [https://www.linuxjournal.com/article/6345](https://www.linuxjournal.com/article/6345)
17. [https://jishuin.proginn.com/p/763bfbd47570](https://jishuin.proginn.com/p/763bfbd47570)

